{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90d4b82",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Inception \n",
    "\n",
    "### Going Deeper with Convolutions (Szegedy C. et al., 2014/2015)\n",
    "\n",
    "*The main hallmark of this architecture is the __improved utilization\n",
    "of the computing resources inside the network__. This was achieved by a carefully\n",
    "crafted design that allows for __increasing the depth and width of the network__ while\n",
    "keeping the computational budget constant. To optimize quality, the architectural\n",
    "decisions were based on the __Hebbian principle and the intuition of multi-scale\n",
    "processing__.*\n",
    "\n",
    "Hebbian principle in a nutshell: \"Cells that fire together wire together.\"\n",
    "\n",
    "[Paper](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "\n",
    "*...Inception, which derives its name from the Network in network paper by Lin et al\n",
    "in conjunction with the famous “we need to go deeper” internet meme*\n",
    "\n",
    "<img src=\"../assets/13_inception.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a87482",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import netron\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pretrainedmodels\n",
    "\n",
    "assert torch.cuda.is_available() is True\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b578e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch : 1.10.2\n",
      "ignite: 0.4.8\n",
      "numpy : 1.22.1\n",
      "netron: 5.7.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -p torch,ignite,numpy,netron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15c50e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Goals:\n",
    "\n",
    "* Implement multi-scale features perception\n",
    "* Make the model more computationally efficient and deep\n",
    "\n",
    "<img src=\"../assets/3_inception_noses.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0a04b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Inception blocks\n",
    "<img src=\"../assets/1_inception_naive.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae8f0d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBlock(nn.Module):\n",
    "    def __init__(self, inp_ch: int = 256, out_ch: tuple = (128, 192, 96)) -> None:\n",
    "        super().__init__()\n",
    "        self.cov1x1 = nn.Conv2d(in_channels=inp_ch, out_channels=out_ch[0], kernel_size=1)\n",
    "        self.cov3x3 = nn.Conv2d(in_channels=inp_ch, out_channels=out_ch[1], kernel_size=3, padding=1)\n",
    "        self.cov5x5 = nn.Conv2d(in_channels=inp_ch, out_channels=out_ch[2], kernel_size=5, padding=2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=(1, 1), padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        branch1 = F.relu(self.cov1x1(x))\n",
    "        branch2 = F.relu(self.cov3x3(x))\n",
    "        branch3 = F.relu(self.cov5x5(x))\n",
    "        branch4 = self.maxpool(x)\n",
    "        out = torch.cat((branch1, branch2, branch3, branch4), 1)\n",
    "        print(f'Total feature maps: {out.shape[1]} of size: {out.shape[2:]}')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7191ebe4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature maps: 672 of size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "naive_b = NaiveBlock().eval()\n",
    "x = torch.Tensor(np.random.normal(size=(256, 28, 28)))\n",
    "model_path = os.path.join('onnx_graphs', 'naive_inception_block.onnx')\n",
    "torch.onnx.export(naive_b, torch.unsqueeze(x, 0), model_path,\n",
    "                  input_names=['input'], output_names=['output'], opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbfdc7b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "netron.start(model_path, 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e10ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Number of operations for each branch of NaiveBlock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e23912",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_conv_ops(kernel, output_channel, input_shape):\n",
    "    return np.prod([*kernel, output_channel, *input_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726a27aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 25690112 346816512 481689600]\n",
      "[0.03007519 0.40601504 0.56390977]\n",
      "Total operations: 854.196 M\n"
     ]
    }
   ],
   "source": [
    "ops_cnt = np.array((count_conv_ops((1, 1), 128, (256, 28, 28)),\n",
    "                    count_conv_ops((3, 3), 192, (256, 28, 28)),\n",
    "                    count_conv_ops((5, 5), 96, (256, 28, 28))))\n",
    "print(\"%s\\n%s\" % (ops_cnt, ops_cnt/np.sum(ops_cnt)))\n",
    "print('Total operations: %.3f M' % (np.sum(ops_cnt)/1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81e25b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*...in our setting, __1 × 1 convolutions have dual purpose: most critically, they\n",
    "are used mainly as dimension reduction modules__ to remove computational bottlenecks, that would\n",
    "otherwise limit the size of our networks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db20ddb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/8_inception.png\" width=\"300\">\n",
    "\n",
    "<img src=\"../assets/1_inception_reduction.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d32619c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, inp_ch: int = 256, out_ch: tuple = (128, 64, 192, 64, 96, 64)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch1 = BasicConv2d(\n",
    "            in_channels=inp_ch, out_channels=out_ch[0], kernel_size=1\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(inp_ch, out_ch[1], kernel_size=1),\n",
    "            BasicConv2d(out_ch[1], out_ch[2], kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(inp_ch, out_ch[3], kernel_size=1),\n",
    "            BasicConv2d(out_ch[3], out_ch[4], kernel_size=5, padding=2)\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=(1, 1)),\n",
    "            BasicConv2d(inp_ch, out_ch[5], kernel_size=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        out = torch.cat((branch1, branch2, branch3, branch4), 1)\n",
    "        print(f'Total feature maps: {out.shape[1]} of size: {out.shape[2:]}')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82837acb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature maps: 480 of size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "inc_block = InceptionBlock().eval()\n",
    "x = torch.Tensor(np.random.normal(size=(256, 28, 28)))\n",
    "model_path = os.path.join('onnx_graphs', 'inc_block.onnx')\n",
    "torch.onnx.export(inc_block, torch.unsqueeze(x, 0), model_path, \n",
    "                  input_names=['input'], output_names=['output'], opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8d18e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "netron.start(model_path, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb69bc72",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch1: 25690112 0.095\n",
      "Branch2: [12845056 86704128] 0.367\n",
      "Branch3: [ 12845056 120422400] 0.491\n",
      "Branch4: [12845056] 0.047\n",
      "Total operations: 271.352 M\n"
     ]
    }
   ],
   "source": [
    "ops_cnt = np.array((count_conv_ops((1, 1), 128, (256, 28, 28)),\n",
    "                    \n",
    "                    count_conv_ops((1, 1), 64, (256, 28, 28)),\n",
    "                    count_conv_ops((3, 3), 192, (64, 28, 28)),\n",
    "                    \n",
    "                    count_conv_ops((1, 1), 64, (256, 28, 28)),\n",
    "                    count_conv_ops((5, 5), 96, (64, 28, 28)), \n",
    "                    \n",
    "                    count_conv_ops((1, 1), 64, (256, 28, 28)), \n",
    "                    \n",
    "                  ))\n",
    "\n",
    "print(\"Branch1: %s %.3f\"  %  (ops_cnt[0], (ops_cnt/np.sum(ops_cnt))[0]))\n",
    "print(\"Branch2: %s %.3f\" % (ops_cnt[1:3], sum((ops_cnt/np.sum(ops_cnt))[1:3])))\n",
    "print(\"Branch3: %s %.3f\" % (ops_cnt[3:5], sum((ops_cnt/np.sum(ops_cnt))[3:5])))\n",
    "print(\"Branch4: %s %.3f\" % (ops_cnt[5:], sum((ops_cnt/np.sum(ops_cnt))[5:])))\n",
    "print('Total operations: %.3f M' % (np.sum(ops_cnt)/1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adedc16",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Inception V1:\n",
    "\n",
    "* 9 stacked inception modules, 22 layers (27 with pooling)\n",
    "* No fully connected layers. Average pooling +  Linear layer (Improved the top-1 accuracy by about 0.6%)\n",
    "* InceptionV1 loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2\n",
    "\n",
    "\n",
    "<img src=\"../assets/7_inception.png\" width=\"800\">\n",
    "\n",
    "<img src=\"../assets/2_inception.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3f735",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/4_inception.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b93e00a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Torch [implementation](https://github.com/pytorch/vision/blob/435eddf7a8200cc26338036a0a5f7db067ac7b0c/torchvision/models/googlenet.py#L28)\n",
    "\n",
    "\n",
    "```python\n",
    "class GoogLeNet(nn.Module):\n",
    "    __constants__ = [\"aux_logits\", \"transform_input\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 1000,\n",
    "        aux_logits: bool = True,\n",
    "        transform_input: bool = False,\n",
    "        init_weights: Optional[bool] = None,\n",
    "        blocks: Optional[List[Callable[..., nn.Module]]] = None,\n",
    "        dropout: float = 0.2,\n",
    "        dropout_aux: float = 0.7,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, InceptionAux]\n",
    "        if init_weights is None:\n",
    "            warnings.warn(\n",
    "                \"The default weight initialization of GoogleNet will be changed in future releases of \"\n",
    "                \"torchvision. If you wish to keep the old behavior (which leads to long initialization times\"\n",
    "                \" due to scipy/scipy#11299), please set init_weights=True.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            init_weights = True\n",
    "        assert len(blocks) == 3\n",
    "        conv_block = blocks[0]\n",
    "        inception_block = blocks[1]\n",
    "        inception_aux_block = blocks[2]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "\n",
    "        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "        self.conv2 = conv_block(64, 64, kernel_size=1)\n",
    "        self.conv3 = conv_block(64, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = inception_block(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = inception_block(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = inception_block(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = inception_block(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = inception_block(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = inception_block(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = inception_block(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = inception_block(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = inception_block(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        if aux_logits:\n",
    "            self.aux1 = inception_aux_block(512, num_classes, dropout=dropout_aux)\n",
    "            self.aux2 = inception_aux_block(528, num_classes, dropout=dropout_aux)\n",
    "        else:\n",
    "            self.aux1 = None  # type: ignore[assignment]\n",
    "            self.aux2 = None  # type: ignore[assignment]\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa58c52",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Inception block in torchvision does not contain 5x5 convolutions:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        ch1x1: int,\n",
    "        ch3x3red: int,\n",
    "        ch3x3: int,\n",
    "        ch5x5red: int,\n",
    "        ch5x5: int,\n",
    "        pool_proj: int,\n",
    "        conv_block: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            conv_block(in_channels, ch5x5red, kernel_size=1),\n",
    "            # Here, kernel_size=3 instead of kernel_size=5 is a known bug.\n",
    "            # Please see https://github.com/pytorch/vision/issues/906 for details.\n",
    "            conv_block(ch5x5red, ch5x5, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c00d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Further work\n",
    "\n",
    "#### Inception V2: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (Ioffe, S., Szegedy, C.2015)\n",
    "[Paper](https://arxiv.org/pdf/1502.03167.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3374bd6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* *The main difference to the network described in (Szegedy et al., 2014) is that the 5 × 5 convolutional layers are replaced by two consecutive layers of 3 × 3 convolutions with up to 128 filters.*\n",
    "\n",
    "* Small arhitecture changes: more inception modules, avg pool and max pool mix, strides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb25930e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BasicConv2dBN(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.bn(self.conv(x))\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "\n",
    "class InceptionBlockV2(nn.Module):\n",
    "    def __init__(self, inp_ch: int = 256, out_ch: tuple = (128, 64, 192, 64, 96, 64)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch1 = BasicConv2dBN(\n",
    "            in_channels=inp_ch, out_channels=out_ch[0], kernel_size=1\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2dBN(inp_ch, out_ch[1], kernel_size=1),\n",
    "            BasicConv2dBN(out_ch[1], out_ch[2], kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2dBN(inp_ch, out_ch[3], kernel_size=1),\n",
    "            BasicConv2dBN(out_ch[3], out_ch[4], kernel_size=3, padding=1),\n",
    "            BasicConv2dBN(out_ch[4], out_ch[4], kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=(1, 1)),\n",
    "            BasicConv2dBN(inp_ch, out_ch[5], kernel_size=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        out = torch.cat((branch1, branch2, branch3, branch4), 1)\n",
    "        print(f'Total feature maps: {out.shape[1]} of size: {out.shape[2:]}')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "666940bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature maps: 480 of size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "inc2_block = InceptionBlockV2().eval()\n",
    "x = torch.Tensor(np.random.normal(size=(256, 28, 28)))\n",
    "model_path = os.path.join('onnx_graphs', 'inc2_block.onnx')\n",
    "torch.onnx.export(inc2_block, torch.unsqueeze(x, 0), model_path, \n",
    "                  input_names=['input'], output_names=['output'], opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3e419",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "netron.start(model_path, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87fd7823",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch1: 25690112 0.099\n",
      "Branch2: [12845056 86704128] 0.384\n",
      "Branch3: [12845056 43352064 65028096] 0.467\n",
      "Branch4: [12845056] 0.050\n",
      "Total operations: 259.310 M\n"
     ]
    }
   ],
   "source": [
    "ops_cnt = np.array((count_conv_ops((1, 1), 128, (256, 28, 28)),\n",
    "\n",
    "                    count_conv_ops((1, 1), 64, (256, 28, 28)),\n",
    "                    count_conv_ops((3, 3), 192, (64, 28, 28)),\n",
    "\n",
    "                    count_conv_ops((1, 1), 64, (256, 28, 28)),\n",
    "                    count_conv_ops((3, 3), 96, (64, 28, 28)),\n",
    "                    count_conv_ops((3, 3), 96, (96, 28, 28)),\n",
    "\n",
    "                    count_conv_ops((1, 1), 64, (256, 28, 28)),\n",
    "\n",
    "                    ))\n",
    "\n",
    "print(\"Branch1: %s %.3f\"  %  (ops_cnt[0], (ops_cnt/np.sum(ops_cnt))[0]))\n",
    "print(\"Branch2: %s %.3f\" % (ops_cnt[1:3], sum((ops_cnt/np.sum(ops_cnt))[1:3])))\n",
    "print(\"Branch3: %s %.3f\" % (ops_cnt[3:6], sum((ops_cnt/np.sum(ops_cnt))[3:6])))\n",
    "print(\"Branch4: %s %.3f\" % (ops_cnt[6:], sum((ops_cnt/np.sum(ops_cnt))[6:])))\n",
    "print('Total operations: %.3f M' % (np.sum(ops_cnt)/1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb578d0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Inception V3: Rethinking the Inception Architecture for Computer Vision (Szegedy C. et al., 2016)\n",
    "[Paper](https://arxiv.org/pdf/1512.00567.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b9ae8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Principles and optimization ideas that that proved to be useful for scaling up convolution networks:\n",
    "   \n",
    "   * Avoid representational bottlenecks, especially early in the network\n",
    "   * Increasing the activations per tile in a convolutional network allows for more disentangled features\n",
    "   * Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power.  Convolutions with filters larger 3 × 3 a might not be generally useful as they can always be reduced into a sequence of 3 × 3 convolutional layers.\n",
    "   * Optimal performance of the network can be reached by balancing the number of filters per stage and the depth of the network\n",
    "  \n",
    "Use them judiciously in ambiguous situations only.\n",
    "\n",
    "* Replacing n × n convolutions by a 1 × n convolution followed by a n × 1 gives very good results on medium grid-sizes (On m × m feature maps, where m ranges between 12 and 20)\n",
    "\n",
    "* Auxiliary classifiers did not result in improved convergence early in the training. In general they act as regularizers.\n",
    "\n",
    "* Label smoothing regularization (LSR) - noising one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd512b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/9_inception.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94d156",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/10_inception.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40059b94",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "#### Inception V4: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning (Szegedy C. et al., 2017)\n",
    "\n",
    "[Paper](https://arxiv.org/abs/1602.07261)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362be88b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/12_inception.png\" width=\"220\">\n",
    "\n",
    "* Residual connections leads to dramatically improved training speed for the Inception architecture\n",
    "* Inception-v4: a pure Inception variant without residual connections with roughly the same recognition performance as Inception-ResNet-v2.\n",
    "\n",
    "<img src=\"../assets/11_inception.png\" width=\"440\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b60e1e4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('googlenet', 'inception', 'inception_v3')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(arch for arch in dir(torchvision.models) if 'inception' in arch or 'google' in arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "766688e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bninception', 'inceptionresnetv2', 'inceptionv3', 'inceptionv4')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(x for x in dir(pretrainedmodels) if 'incept' in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8fde03",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Your training code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698fbf9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define data transformation pipeline.\n",
    "\n",
    "\n",
    "# Initialize dataset and dataloaders.\n",
    "\n",
    "\n",
    "# Initialize pretrained network, replace Linear layer with a new one for your dataset.\n",
    "\n",
    "\n",
    "# Initialize optimizer, loss function and training procedure with handlers/callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02e731",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References\n",
    "\n",
    "* http://cs231n.stanford.edu/slides/2021/lecture_9.pdf\n",
    "* https://cs231n.github.io/convolutional-networks/\n",
    "* https://www.cs.colostate.edu/~dwhite54/InceptionNetworkOverview.pdf\n",
    "* https://onnx.ai/\n",
    "* https://leimao.github.io/blog/Label-Smoothing/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
