{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a19a156",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Understanding the difficulty of training deep feedforward neural networks (Glorot, X.,  Bengio, Y., 2010)\n",
    "[Paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef1f81",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "До исследования Йошуа Бенджи и Глоро Ксавьера инициализация весов нейронной сети осуществлялась с помощью равномерного распределения при следующих параметрах:\n",
    "$$w_i \\sim U[-\\frac{1}{ \\sqrt{n_{out}}}, \\frac{1}{ \\sqrt{n_{out}}} ] (*) $$ \n",
    "\n",
    "Рассмотрим рассуждение авторов:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba15404",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Пусть:\n",
    "    * Значение активации одного нейрона: $ y = w^Tx + b$;\n",
    "    \n",
    "    * $n_{out}$ - количество нейронов на последнем слое;\n",
    "    \n",
    "    \n",
    "2. Тогда $Var(y)$ зависит только от $$Var(y) = Var(w^Tx + b) = w^Tx$$\n",
    "\n",
    "\n",
    "3. Предположим, \n",
    "\n",
    "    1) Сл. вел. $w$ и $x$ - i.i.d и взаимно независимы;\n",
    "    \n",
    "    2) Используем __`симметричную функцию активации`__ и инициализируем веса модели со средним значением 0;\n",
    "\n",
    "\n",
    "Раскроем $Var(y)$ как дисперсию произведения двух независимых случайных величин $w_ix_i$:\n",
    "\n",
    "$$Var(y) = Var(w_ix_i) = E({{w_i}^2 {x_i}^2})- E(w_i x_i)^2 = E(X_i)^2Var(w_i) + E(w_i)^2Var(X_i) + Var(w_i)Var(x_i) $$ \n",
    "\n",
    "Что из 2) ведет к:\n",
    "\n",
    "$$Var(y) = Var(w_ix_i) = Var(w_i)Var(x_i)$$\n",
    "\n",
    "В этом случае, если  $x_i$ и $w_i$ инициализируются независимо друг от друга ( 1) ), то:\n",
    "\n",
    "$$Var(y) = Var(\\sum_{i}^{n_{out}}{y_i}) = Var(\\sum_{i}^{n_{out}}{w_ix_i}) = n_{out}Var(w_i)Var(x_i),$$\n",
    "\n",
    "\n",
    "\n",
    "Т.е. получается, дисперсия выхода пропорциональна дисперсии входа с коэффициентом $n_{out}Var(w_i)$\n",
    "\n",
    "\n",
    "Тогда, дисперсия весов, сгенерированных непрерывным равномерным распределением (*):\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)\n",
    "\n",
    "$$Var(w_i) =  \\frac{1}{12}(\\frac{1}{ \\sqrt{n_{out}}} + \\frac{1}{ \\sqrt{n_{out}}})^2 = \\frac{1}{3n_{out}}. $$\n",
    "\n",
    "Следовательно:\n",
    "\n",
    "$$n_{out}Var(w_i) = \\frac{1}{3}$$\n",
    "\n",
    "Т.е. дисперсия результата каждого слоя уменьшается в 3 раза. __Что это означает? Vanishing signal!__\n",
    "\n",
    "В общем случае:\n",
    "$$\n",
    "\\begin{equation}\n",
    "  n_{out}Var(w_i)\n",
    "    \\begin{cases}\n",
    "      < 1 & \\text{, vanishing signal,}\\\\\n",
    "      = 1 & \\text{, equal variance from one layer to another,}\\\\\n",
    "      > 1 & \\text{, exploiding signal;}\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Для беспрепятственного распространения значений активационной функции и градиента по всей сети необходимо удовлетворять двум условиям:\n",
    "\n",
    "1. Дисперсия текущего слоя = дисперсии предыдущего слоя.\n",
    "2. М.о.текущего слоя = м.о. предыдущего слоя.\n",
    "\n",
    "Для этого предложено инициализировать веса очередного слоя симметричным распределением с дисперсией:\n",
    "\n",
    "$$Var(w_i) = \\frac{2}{n_{in} + n_{out}}$$\n",
    "\n",
    "Для равномерной инициализации весов это приводит к распределению вида:\n",
    "\n",
    "$$w_i \\sim U[-\\frac{\\sqrt{6}}{ \\sqrt{n_{in}+n_{out}}}, \\frac{\\sqrt{6}}{ \\sqrt{n_{in}+n_{out}}} ]$$ \n",
    "\n",
    "Что и является Xavier-инициализацией весов на случай $U[a, b]-$распределения.\n",
    "\n",
    "Ограничением Xavier является то, что он основывается на функции активации `tanh, симметричной относительно 0`. Вопрос, как лучше инициализировать веса в других случаях с несимметричными относительно 0 функциями, например, с `ReLU`, оставался открытым до выхода работы __(He, Kaiming, et al., 2015)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0362344",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Поэтому, если приходится обучать модели \"from scratch\", можно следовать правилу:\n",
    "\n",
    "1. Для симметричных функция активации - __Glorot Xavier-инициализация__.\n",
    "2. Для несимметричных функций активации - __Kaiming He-инициализация__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e19ee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References:\n",
    "\n",
    "* [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "* [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852v1.pdf)\n",
    "* https://www.deeplearning.ai/ai-notes/initialization/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f6023",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Fashion MNIST example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1e9b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7318535",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c8f8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}