{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2839179a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### EfficientNets \n",
    "\n",
    "\n",
    "### EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan M., Le Q., 2019, Google Research, Brain Team)\n",
    "\n",
    "* arXiv manuscript updates in 2020\n",
    "\n",
    "[Paper](https://arxiv.org/abs/1905.11946)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395c2380",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available() is True\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f27847",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.10.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47605aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41870419",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* One of the first systematic approaches to the network scaling question which basically is a combinatorics problem.\n",
    "\n",
    "__Basic scaling types__:\n",
    "* depth, i.e. number of layers\n",
    "* width, i.e. number of channels\n",
    "* resolution\n",
    "\n",
    "__What was before__:\n",
    "\n",
    "* Single dimension scaling, e.g. Wide ResNet.\n",
    "\n",
    "__Searching algorithm__\n",
    "\n",
    "* Neural architecture search (NAS) [Paper](https://arxiv.org/abs/1611.01578)\n",
    "\n",
    "<img src=\"../assets/18_mobilenet.png\" width=\"470\">\n",
    "\n",
    "__How to scale any convnet efficiently?__\n",
    "\n",
    "* Simply scaling each of dimensions with constant ratio. The idea was named as __compound scaling method__.\n",
    "* Details: \n",
    "    * To reduce the design space researchers restricted that all layers must be scaled uniformly with constant ratio.\n",
    "    * Scaling up any dimension of network width, depth, or resolution improves accuracy, but the accuracy gain diminishes for bigger models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417dc14",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/1_efficientnet.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ef8a8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__Compound scaling__\n",
    "\n",
    "\n",
    "* Note: compound scaling is model-independent.\n",
    "\n",
    "compund scaling coefficient: $\\phi$;\\\n",
    "depth: $d=\\alpha^\\phi$;\\\n",
    "width: $w=\\beta^\\phi$;\\\n",
    "resolution: $r=\\gamma^\\phi$.\n",
    "\n",
    "$$\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 â‰ˆ 2, \\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1$$\n",
    "\n",
    "Constraints explanation: for any new $\\phi$, the total FLOPS will approximately increase by $2^\\phi$ because scaling a ConvNet with $d,w,r$ will approximately increase FLOPS by $(\\alpha \\cdot \\beta^2 \\cdot \\gamma^2)^\\phi$.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "Considering constraints above:\n",
    "1. $\\phi$ - const, searching  $\\alpha, \\beta, \\gamma$;\\\n",
    "2. $\\alpha, \\beta, \\gamma$ - const, searching best $\\phi$;\\\n",
    "3. Return to 1.\n",
    "\n",
    "EfficientNet-B0: $\\alpha=1.2, \\beta=1.1, \\gamma=1.15$.\n",
    "\n",
    "\n",
    "\n",
    "__EfficientNet__\n",
    "\n",
    "* Proposed model was optiized with compound scaling.\n",
    "\n",
    "* NAS optimization goal for model $m$: \n",
    "$$ ACC(m) * \\left[\\frac{F(m)}{T}\\right]^\\omega$$\n",
    "\n",
    "$ACC(m)$ - model accuracy;\\\n",
    "$F(m)$ - model FLOPS;\\\n",
    "$T$ - the target FLOPS, 400M;\\\n",
    "$\\omega=$-0.07, a hyperparameter for controlling the trade-off between accuracy and FLOPS.\n",
    "\n",
    "* Main building block  - MBConv, mobile inverted bottleneck.\n",
    "* [SiLU](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html) (Swish) activation\n",
    "* [AutoAugment](http://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html) algorithm which considers augmentation policy as a discrete search problem.\n",
    "* [Stochastic depth](https://paperswithcode.com/method/stochastic-depth) with survival probability 0.8.It shrinks the depth of a network during training, while keeping it unchanged during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273f015",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/2_efficientnet.png\" width=\"500\">\n",
    "\n",
    "<img src=\"../assets/3_efficientnet.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef42ab2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```\n",
    "      Arch           w,   d,   r,  dropout\n",
    "'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
    "'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
    "'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
    "'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
    "'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
    "'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
    "'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
    "'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
    "'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n",
    "'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb1033",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Torch [implementation](https://github.com/pytorch/vision/blob/f40c8df02c197d1a9e194210e40dee0e6a6cb1c3/torchvision/models/efficientnet.py#L152)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f94136",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Your training code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710c5fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define data transformation pipeline.\n",
    "\n",
    "\n",
    "# Initialize dataset and dataloaders.\n",
    "\n",
    "\n",
    "# Initialize pretrained network, replace Linear layer with a new one for your dataset.\n",
    "\n",
    "\n",
    "# Initialize optimizer, loss function and training procedure with handlers/callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864cc8be",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References\n",
    "\n",
    "* https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet\n",
    "* https://onnx.ai/\n",
    "* https://pytorch.org/docs/stable/index.html\n",
    "* https://paperswithcode.com/method/autoaugment\n",
    "* http://pytorch.org/vision/main/generated/torchvision.transforms.AutoAugment.html\n",
    "* https://github.com/4uiiurz1/pytorch-auto-augment\n",
    "* https://paperswithcode.com/method/stochastic-depth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
