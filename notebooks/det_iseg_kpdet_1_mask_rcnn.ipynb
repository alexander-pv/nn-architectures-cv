{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015376a0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Mask R-CNN architecture\n",
    "\n",
    "\n",
    "### Mask R-CNN (He K. et al., 2017)\n",
    "\n",
    "[Paper](https://arxiv.org/abs/1703.06870)\n",
    "\n",
    "*...conceptually simple, flexible, and general framework for object instance segmentation...Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework.*\n",
    "\n",
    "* Object detection\n",
    "* Instance segmentation\n",
    "* Keypoint detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7b4ed8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import plotly.offline as offline\n",
    "\n",
    "assert torch.cuda.is_available() is True\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad4532d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch      : 1.10.2\n",
      "torchvision: 0.11.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -p torch,torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcda7b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's recall Faster RCNN inference scheme:\n",
    "\n",
    "<img src=\"../assets/3_faster_rcnn.svg\" width=\"450\">\n",
    "\n",
    "Faster RCNN:\n",
    "\n",
    "* Backbone/feature extractor\n",
    "\n",
    "\n",
    "* Regional Proposal Network\n",
    "\n",
    "\n",
    "* Fully connected layers, classifier and regressor heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61821c60",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mask RCNN updates:\n",
    "\n",
    "* Third mask layer: $N \\times K \\times [m \\times m]$, \n",
    "\n",
    "where $K$ - the number of classes, $m \\times m - $ binary mask for each class, $N - $ the number of proposals.\n",
    "(default m=28)\n",
    "\n",
    "\n",
    "Mask head:\n",
    "<img src=\"../assets/7_mask_rcnn.svg\" width=\"650\">\n",
    "\n",
    "[Deconv(bad name)/TransposedConv](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)\n",
    "\n",
    "* Mask layer is following the convolutional layer (feature map spatial structure property).\n",
    "\n",
    "\n",
    "* RoI Align instead of RoI Pool "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539f021",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://erdem.pl/4482c23c18077e16a560fb6add556cc7/ROI-pooling.gif\" width=\"450\">\n",
    "\n",
    "\n",
    "<img src=\"../assets/1_mask_rcnn.svg\" width=\"950\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ebad42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Performs Region of Interest (RoI) Align operator with average pooling, as described in Mask R-CNN.\n",
      "\n",
      "    Args:\n",
      "        input (Tensor[N, C, H, W]): The input tensor, i.e. a batch with ``N`` elements. Each element\n",
      "            contains ``C`` feature maps of dimensions ``H x W``.\n",
      "            If the tensor is quantized, we expect a batch size of ``N == 1``.\n",
      "        boxes (Tensor[K, 5] or List[Tensor[L, 4]]): the box coordinates in (x1, y1, x2, y2)\n",
      "            format where the regions will be taken from.\n",
      "            The coordinate must satisfy ``0 <= x1 < x2`` and ``0 <= y1 < y2``.\n",
      "            If a single Tensor is passed, then the first column should\n",
      "            contain the index of the corresponding element in the batch, i.e. a number in ``[0, N - 1]``.\n",
      "            If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i\n",
      "            in the batch.\n",
      "        output_size (int or Tuple[int, int]): the size of the output (in bins or pixels) after the pooling\n",
      "            is performed, as (height, width).\n",
      "        spatial_scale (float): a scaling factor that maps the input coordinates to\n",
      "            the box coordinates. Default: 1.0\n",
      "        sampling_ratio (int): number of sampling points in the interpolation grid\n",
      "            used to compute the output value of each pooled output bin. If > 0,\n",
      "            then exactly ``sampling_ratio x sampling_ratio`` sampling points per bin are used. If\n",
      "            <= 0, then an adaptive number of grid points are used (computed as\n",
      "            ``ceil(roi_width / output_width)``, and likewise for height). Default: -1\n",
      "        aligned (bool): If False, use the legacy implementation.\n",
      "            If True, pixel shift the box coordinates it by -0.5 for a better alignment with the two\n",
      "            neighboring pixel indices. This version is used in Detectron2\n",
      "\n",
      "    Returns:\n",
      "        Tensor[K, C, output_size[0], output_size[1]]: The pooled RoIs.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(torchvision.ops.roi_align.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5541a10f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Loss:\n",
    "\n",
    "$$L = L_{cls} + L_{box} + L_{mask}$$\n",
    "\n",
    "$L_{mask}$ class-averaged binary cross entropy loss;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Multiple feature maps\n",
    "\n",
    "* Feature pyramid network, FPN\n",
    "\n",
    "FPN general scheme:\n",
    "<img src=\"../assets/6_mask_rcnn.png\" width=\"450\">\n",
    "\n",
    "[pytorch upsample](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ac7a6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training scheme:\n",
    "    \n",
    "<img src=\"../assets/2_mask_rcnn.svg\" width=\"650\">\n",
    "    \n",
    "\n",
    "Inference scheme:\n",
    "\n",
    "<img src=\"../assets/3_mask_rcnn.svg\" width=\"450\">\n",
    "\n",
    "\n",
    "\n",
    "[Matterport](https://github.com/matterport/Mask_RCNN) training and inference implementation:\n",
    "\n",
    "<img src=\"../assets/5_mask_rcnn.svg\" width=\"950\">\n",
    "\n",
    "<img src=\"../assets/4_mask_rcnn.svg\" width=\"1050\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e6ef1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Implements Mask R-CNN.\n",
      "\n",
      "    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n",
      "    image, and should be in 0-1 range. Different images can have different sizes.\n",
      "\n",
      "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
      "\n",
      "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
      "    containing:\n",
      "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n",
      "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
      "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
      "        - masks (UInt8Tensor[N, H, W]): the segmentation binary masks for each instance\n",
      "\n",
      "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
      "    losses for both the RPN and the R-CNN, and the mask loss.\n",
      "\n",
      "    During inference, the model requires only the input tensors, and returns the post-processed\n",
      "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
      "    follows:\n",
      "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n",
      "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
      "        - labels (Int64Tensor[N]): the predicted labels for each image\n",
      "        - scores (Tensor[N]): the scores or each prediction\n",
      "        - masks (UInt8Tensor[N, 1, H, W]): the predicted masks for each instance, in 0-1 range. In order to\n",
      "          obtain the final segmentation masks, the soft masks can be thresholded, generally\n",
      "          with a value of 0.5 (mask >= 0.5)\n",
      "\n",
      "    Args:\n",
      "        backbone (nn.Module): the network used to compute the features for the model.\n",
      "            It should contain a out_channels attribute, which indicates the number of output\n",
      "            channels that each feature map has (and it should be the same for all feature maps).\n",
      "            The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
      "        num_classes (int): number of output classes of the model (including the background).\n",
      "            If box_predictor is specified, num_classes should be None.\n",
      "        min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n",
      "        max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n",
      "        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
      "            They are generally the mean values of the dataset on which the backbone has been trained\n",
      "            on\n",
      "        image_std (Tuple[float, float, float]): std values used for input normalization.\n",
      "            They are generally the std values of the dataset on which the backbone has been trained on\n",
      "        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
      "            maps.\n",
      "        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
      "        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
      "        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
      "        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
      "        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
      "        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
      "        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
      "            considered as positive during training of the RPN.\n",
      "        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
      "            considered as negative during training of the RPN.\n",
      "        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
      "            for computing the loss\n",
      "        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
      "            of the RPN\n",
      "        rpn_score_thresh (float): during inference, only return proposals with a classification score\n",
      "            greater than rpn_score_thresh\n",
      "        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
      "            the locations indicated by the bounding boxes\n",
      "        box_head (nn.Module): module that takes the cropped feature maps as input\n",
      "        box_predictor (nn.Module): module that takes the output of box_head and returns the\n",
      "            classification logits and box regression deltas.\n",
      "        box_score_thresh (float): during inference, only return proposals with a classification score\n",
      "            greater than box_score_thresh\n",
      "        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
      "        box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
      "        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
      "            considered as positive during training of the classification head\n",
      "        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
      "            considered as negative during training of the classification head\n",
      "        box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
      "            classification head\n",
      "        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
      "            of the classification head\n",
      "        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n",
      "            bounding boxes\n",
      "        mask_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
      "             the locations indicated by the bounding boxes, which will be used for the mask head.\n",
      "        mask_head (nn.Module): module that takes the cropped feature maps as input\n",
      "        mask_predictor (nn.Module): module that takes the output of the mask_head and returns the\n",
      "            segmentation mask logits\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> import torch\n",
      "        >>> import torchvision\n",
      "        >>> from torchvision.models.detection import MaskRCNN\n",
      "        >>> from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
      "        >>>\n",
      "        >>> # load a pre-trained model for classification and return\n",
      "        >>> # only the features\n",
      "        >>> backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
      "        >>> # MaskRCNN needs to know the number of\n",
      "        >>> # output channels in a backbone. For mobilenet_v2, it's 1280\n",
      "        >>> # so we need to add it here\n",
      "        >>> backbone.out_channels = 1280\n",
      "        >>>\n",
      "        >>> # let's make the RPN generate 5 x 3 anchors per spatial\n",
      "        >>> # location, with 5 different sizes and 3 different aspect\n",
      "        >>> # ratios. We have a Tuple[Tuple[int]] because each feature\n",
      "        >>> # map could potentially have different sizes and\n",
      "        >>> # aspect ratios\n",
      "        >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
      "        >>>                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
      "        >>>\n",
      "        >>> # let's define what are the feature maps that we will\n",
      "        >>> # use to perform the region of interest cropping, as well as\n",
      "        >>> # the size of the crop after rescaling.\n",
      "        >>> # if your backbone returns a Tensor, featmap_names is expected to\n",
      "        >>> # be ['0']. More generally, the backbone should return an\n",
      "        >>> # OrderedDict[Tensor], and in featmap_names you can choose which\n",
      "        >>> # feature maps to use.\n",
      "        >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
      "        >>>                                                 output_size=7,\n",
      "        >>>                                                 sampling_ratio=2)\n",
      "        >>>\n",
      "        >>> mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
      "        >>>                                                      output_size=14,\n",
      "        >>>                                                      sampling_ratio=2)\n",
      "        >>> # put the pieces together inside a MaskRCNN model\n",
      "        >>> model = MaskRCNN(backbone,\n",
      "        >>>                  num_classes=2,\n",
      "        >>>                  rpn_anchor_generator=anchor_generator,\n",
      "        >>>                  box_roi_pool=roi_pooler,\n",
      "        >>>                  mask_roi_pool=mask_roi_pooler)\n",
      "        >>> model.eval()\n",
      "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
      "        >>> predictions = model(x)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(torchvision.models.detection.mask_rcnn.MaskRCNN.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8279d67",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Keypoint detection: Human Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3d365",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* $K$ classes of each point for an instance\n",
    "* Only 1 point is foreground\n",
    "* Bilinear upsacle increases accuracy\n",
    "* Cross-entropy loss minimizes over an $(2m)^2$-way softmax output\n",
    "\n",
    "<img src=\"../assets/8_mask_rcnn.svg\" width=\"750\">\n",
    "\n",
    "<img src=\"../assets/9_mask_rcnn.svg\" width=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadf77aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Implements Keypoint R-CNN.\n",
      "\n",
      "    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n",
      "    image, and should be in 0-1 range. Different images can have different sizes.\n",
      "\n",
      "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
      "\n",
      "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
      "    containing:\n",
      "\n",
      "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n",
      "            ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
      "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
      "        - keypoints (FloatTensor[N, K, 3]): the K keypoints location for each of the N instances, in the\n",
      "          format [x, y, visibility], where visibility=0 means that the keypoint is not visible.\n",
      "\n",
      "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
      "    losses for both the RPN and the R-CNN, and the keypoint loss.\n",
      "\n",
      "    During inference, the model requires only the input tensors, and returns the post-processed\n",
      "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
      "    follows:\n",
      "\n",
      "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n",
      "            ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
      "        - labels (Int64Tensor[N]): the predicted labels for each image\n",
      "        - scores (Tensor[N]): the scores or each prediction\n",
      "        - keypoints (FloatTensor[N, K, 3]): the locations of the predicted keypoints, in [x, y, v] format.\n",
      "\n",
      "    Args:\n",
      "        backbone (nn.Module): the network used to compute the features for the model.\n",
      "            It should contain a out_channels attribute, which indicates the number of output\n",
      "            channels that each feature map has (and it should be the same for all feature maps).\n",
      "            The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
      "        num_classes (int): number of output classes of the model (including the background).\n",
      "            If box_predictor is specified, num_classes should be None.\n",
      "        min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n",
      "        max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n",
      "        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
      "            They are generally the mean values of the dataset on which the backbone has been trained\n",
      "            on\n",
      "        image_std (Tuple[float, float, float]): std values used for input normalization.\n",
      "            They are generally the std values of the dataset on which the backbone has been trained on\n",
      "        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
      "            maps.\n",
      "        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
      "        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
      "        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
      "        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
      "        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
      "        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
      "        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
      "            considered as positive during training of the RPN.\n",
      "        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
      "            considered as negative during training of the RPN.\n",
      "        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
      "            for computing the loss\n",
      "        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
      "            of the RPN\n",
      "        rpn_score_thresh (float): during inference, only return proposals with a classification score\n",
      "            greater than rpn_score_thresh\n",
      "        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
      "            the locations indicated by the bounding boxes\n",
      "        box_head (nn.Module): module that takes the cropped feature maps as input\n",
      "        box_predictor (nn.Module): module that takes the output of box_head and returns the\n",
      "            classification logits and box regression deltas.\n",
      "        box_score_thresh (float): during inference, only return proposals with a classification score\n",
      "            greater than box_score_thresh\n",
      "        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
      "        box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
      "        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
      "            considered as positive during training of the classification head\n",
      "        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
      "            considered as negative during training of the classification head\n",
      "        box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
      "            classification head\n",
      "        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
      "            of the classification head\n",
      "        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n",
      "            bounding boxes\n",
      "        keypoint_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
      "             the locations indicated by the bounding boxes, which will be used for the keypoint head.\n",
      "        keypoint_head (nn.Module): module that takes the cropped feature maps as input\n",
      "        keypoint_predictor (nn.Module): module that takes the output of the keypoint_head and returns the\n",
      "            heatmap logits\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> import torch\n",
      "        >>> import torchvision\n",
      "        >>> from torchvision.models.detection import KeypointRCNN\n",
      "        >>> from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
      "        >>>\n",
      "        >>> # load a pre-trained model for classification and return\n",
      "        >>> # only the features\n",
      "        >>> backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
      "        >>> # KeypointRCNN needs to know the number of\n",
      "        >>> # output channels in a backbone. For mobilenet_v2, it's 1280\n",
      "        >>> # so we need to add it here\n",
      "        >>> backbone.out_channels = 1280\n",
      "        >>>\n",
      "        >>> # let's make the RPN generate 5 x 3 anchors per spatial\n",
      "        >>> # location, with 5 different sizes and 3 different aspect\n",
      "        >>> # ratios. We have a Tuple[Tuple[int]] because each feature\n",
      "        >>> # map could potentially have different sizes and\n",
      "        >>> # aspect ratios\n",
      "        >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
      "        >>>                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
      "        >>>\n",
      "        >>> # let's define what are the feature maps that we will\n",
      "        >>> # use to perform the region of interest cropping, as well as\n",
      "        >>> # the size of the crop after rescaling.\n",
      "        >>> # if your backbone returns a Tensor, featmap_names is expected to\n",
      "        >>> # be ['0']. More generally, the backbone should return an\n",
      "        >>> # OrderedDict[Tensor], and in featmap_names you can choose which\n",
      "        >>> # feature maps to use.\n",
      "        >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
      "        >>>                                                 output_size=7,\n",
      "        >>>                                                 sampling_ratio=2)\n",
      "        >>>\n",
      "        >>> keypoint_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
      "        >>>                                                          output_size=14,\n",
      "        >>>                                                          sampling_ratio=2)\n",
      "        >>> # put the pieces together inside a KeypointRCNN model\n",
      "        >>> model = KeypointRCNN(backbone,\n",
      "        >>>                      num_classes=2,\n",
      "        >>>                      rpn_anchor_generator=anchor_generator,\n",
      "        >>>                      box_roi_pool=roi_pooler,\n",
      "        >>>                      keypoint_roi_pool=keypoint_roi_pooler)\n",
      "        >>> model.eval()\n",
      "        >>> model.eval()\n",
      "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
      "        >>> predictions = model(x)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(torchvision.models.detection.keypoint_rcnn.KeypointRCNN.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e227f49",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The surprising impact of mask-head architecture on novel class segmentation  (Birodkar V. et al., 2021)\n",
    "[Paper](https://arxiv.org/abs/2104.00613v2)\n",
    "\n",
    "*... we study Mask R-CNN and discover that instead of its default strategy of training the mask-head with a combination of proposals and groundtruth boxes, training the mask-head with only groundtruth boxes dramatically improves its performance on novel classes*\n",
    "\n",
    "### Pointly-Supervised Instance Segmentation (Cheng B., Parkhi O., Kirillov A., 2021)\n",
    "[Paper](https://arxiv.org/pdf/2104.06404v1.pdf)\n",
    "\n",
    "*...existing instance segmentation models developed for full mask supervision, like Mask R-CNN, can be seamlessly trained with the point-based annotation without any major modifications*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23cec1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References:\n",
    "\n",
    "* https://www.youtube.com/watch?v=g7z4mkfRjI4\\\n",
    "* https://www.youtube.com/watch?v=nDPWywWRIRo\n",
    "* https://paperswithcode.com/paper/mask-r-cnn\n",
    "* https://x-engineer.org/bilinear-interpolation/\n",
    "* [FPN](https://arxiv.org/abs/1612.03144)\n",
    "* [FCN](https://arxiv.org/abs/1411.4038)\n",
    "* https://pytorch.org/vision/0.12/_modules/torchvision/models/detection/keypoint_rcnn.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
