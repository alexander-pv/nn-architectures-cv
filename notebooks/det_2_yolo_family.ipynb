{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3962e6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### YOLO object detection architecture\n",
    "\n",
    "\n",
    "### You Only Look Once: Unified, Real-Time Object Detection (Redmon J. et al. , 2016)\n",
    "[Paper](https://arxiv.org/abs/1506.02640)\n",
    "\n",
    "\n",
    "*...we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c56996",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from typing import Tuple, Callable\n",
    "\n",
    "import cv2\n",
    "\n",
    "import netron\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from ignite.engine import create_supervised_evaluator\n",
    "from ignite.metrics import Loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert torch.cuda.is_available() is True\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6acce03",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch  : 1.10.2\n",
      "ignite : 0.4.8\n",
      "numpy  : 1.22.1\n",
      "cv2    : 4.5.5\n",
      "sklearn: 0.24.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -p torch,ignite,numpy,cv2,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31d84e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/1_yolo.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3c03f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* S x S grid on input, default S=7.\n",
    "\n",
    "\n",
    "* If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.\n",
    "\n",
    "\n",
    "* Each grid cell has output tensor $1 x 1 x(5 * B + C)$, $C$ - number of classes, $B$ - numbder of predicted bboxes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05e9c1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Intuition:\n",
    "<img src=\"../assets/3_yolo.svg\" width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "Assumption: *Multiple object centers can't exist in one grid cell.*\n",
    "\n",
    "\n",
    "* Bbox: $(x, y, w, h, p_{object})$\n",
    "\n",
    "    * (x, y) - the center of the box relative to the bounds of the grid cell, \n",
    "    * w & h - relative to the whole image\n",
    "    * $p_{object}$ - the confidence value that a grid cell contains an object\n",
    "    \n",
    "If $W_x, W_y - $ cell size, $C_x, C_y$ - object center coordinates relative to the upper left corner of the cell,  then:\n",
    "\n",
    "$$ x = \\frac{Cx}{W_x}, $$\n",
    "\n",
    "$$ y = \\frac{C_y}{W_y}, $$ \n",
    "\n",
    "$$ h = \\frac{H_{object}}{H}, $$\n",
    "\n",
    "$$ h = \\frac{W_{object}}{W}. $$\n",
    "\n",
    "During taining, a bounding box for a particular cell = (x, y, w, h, 1), if center is in this cell. Empty cells have zero values in output vectors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GoogLeNet-inspired YOLOv1:\n",
    "<img src=\"../assets/2_yolo.png\" width=\"850\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b814d8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "* Multi-part loss fucntion:\n",
    "\n",
    "$Loss_{total} = \\sum_i^{S \\cdot S}L(i)$\n",
    "\n",
    "$L(i) = \\mathbb{1}^{obj}[L_{class}(i) + \\lambda_{coord}L_{coord}(i) + L_{conf}(i)] + \\mathbb{1}^{noobj}\\lambda_{noobj}L_{noobj}(i) $\n",
    "\n",
    "[Loss details](https://arxiv.org/pdf/1506.02640.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49b79bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import metrics\n",
    "import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c98a9395",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ShapesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, n_images: int, img_size: int, seed: int, transform: Callable = None, \n",
    "                 name: str = 'default', verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Map-style ShapesDataset. YOLO version.\n",
    "\n",
    "        :param n_images:          number of images to generate\n",
    "        :param img_size:          crop image size\n",
    "        :param transform:         final crops transformations\n",
    "        :param name:              dataset name\n",
    "        :param verbose:           class versbosity\n",
    "        \"\"\"\n",
    "        self.n_images = n_images\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.shapes2d = utils.RandomShapes(\n",
    "            img_size=self.img_size, w=50, h=50, nms_threshold=0.1, \n",
    "            background_class=False, seed=seed)\n",
    "        self.name = name\n",
    "        self.verbose = verbose\n",
    "        self.fast_ssearch = True\n",
    "\n",
    "        self._generate_dataset()\n",
    "        \n",
    "        self.S = 7\n",
    "        self.B = 2\n",
    "        self.C = len(self.shapes2d.classes)\n",
    "        self.cell_size = self.img_size // self.S\n",
    "        \n",
    "        self.show_color = (250, 0, 0)\n",
    "        self.grid_color = (0, 250, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_gtruth(gtruth: list) -> Tuple[np.ndarray, int]:\n",
    "        gt_bbox = [gt for _, gt in gtruth]\n",
    "        gt_class_id = [idx for idx, _ in gtruth]\n",
    "        return gt_bbox, gt_class_id\n",
    "\n",
    "    @staticmethod\n",
    "    def show_image(image: np.ndarray, title: str) -> None:\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(image)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    def show_bboxes(self, idx: int, centers: bool = True, grid: bool = True) -> None:\n",
    "        image = self._get_image(idx)\n",
    "        boxes = self._get_boxes(idx)\n",
    "        if grid:\n",
    "            image = self._draw_grid(image)\n",
    "        for box in boxes:\n",
    "            image = cv2.rectangle(image, box, self.show_color, 1)\n",
    "            if centers:\n",
    "                image = self._draw_point(image, box[:2] + box[2:]//2)\n",
    "        self.show_image(image, 'Ground truth')\n",
    "        \n",
    "    def show_cell_info(self, row: int, col: int, idx: int) -> None:\n",
    "        _, target = self.__getitem__(idx)\n",
    "        show_boxes = target[row][col][:train_dataset.B*5].reshape(train_dataset.B, 5)\n",
    "        show_labels = target[row][col][train_dataset.B*5:]\n",
    "        class_id = np.argmax(show_labels)\n",
    "        if class_id:\n",
    "            class_name = train_dataset.shapes2d.classes[int(np.argmax(show_labels))]\n",
    "        else:\n",
    "            class_name = 'empty cell'\n",
    "        print(f'\\nCell: [{row}][{col}]')\n",
    "        print(f'Boxes:\\n{show_boxes}\\nClass confidence: {show_labels}\\nName:{class_name}')\n",
    "        \n",
    "    def _draw_grid(self, image: np.ndarray) -> np.ndarray:\n",
    "        for x in list(range(0, self.img_size, self.cell_size)):\n",
    "            for y in list(range(0, self.img_size, self.cell_size)):\n",
    "                w = x + self.cell_size\n",
    "                h = y + self.cell_size\n",
    "                image = cv2.rectangle(image, (x, y, w, h), self.grid_color, 1)\n",
    "        return image \n",
    "    \n",
    "    def _draw_point(self, image: np.ndarray, box: np.ndarray) -> np.ndarray:\n",
    "        image = cv2.circle(image, box, 2, self.show_color, -1)\n",
    "        return image\n",
    "\n",
    "    def _generate_dataset(self) -> None:\n",
    "        print(f'{self.name} Generating dataset...')\n",
    "        self.images_info = {}\n",
    "        for i in range(self.n_images):\n",
    "            bg_color, shape_specs, gtruths = self.shapes2d.generate_shapes()\n",
    "            self.images_info.update({i: {'bg_color': bg_color,\n",
    "                                         'shape_specs': shape_specs,\n",
    "                                         'gtruths': gtruths\n",
    "                                         }\n",
    "                                     }\n",
    "                                    )\n",
    "    \n",
    "    def _encode(self, boxes: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" \n",
    "        Source: https://github.com/motokimura/yolo_v1_pytorch\n",
    "        Encode box coordinates and class labels into a tensor.\n",
    "        \n",
    "        boxes_wh & boxes_xy_center have dims [n, 2]\n",
    "        \n",
    "        :param boxes:           (tensor) [[x, y, w, h]_obj1, ...]\n",
    "        :param labels:          (tensor) [c_obj1, c_obj2, ...]\n",
    "        :return: [S, S, 5 x B + C] tensor\n",
    "        \n",
    "        \"\"\"\n",
    "        S, B, C = self.S, self.B, self.C\n",
    "        N = 5 * B + C\n",
    "\n",
    "        target = torch.zeros(S, S, N)\n",
    "        cell_size = 1.0 / float(S)\n",
    "        boxes_wh =  boxes[:, 2:]  \n",
    "        boxes_xy_center = boxes[:, :2] + boxes[:, 2:]/2.0\n",
    "        \n",
    "        for b in range(boxes.size(0)):\n",
    "            center_xy, wh, label = boxes_xy_center[b], boxes_wh[b], int(labels[b])\n",
    "            ij = (center_xy / cell_size).ceil() - 1.0\n",
    "            i, j = int(ij[0]), int(ij[1]) # y & x index which represents its location on the grid.\n",
    "            x0y0 = ij * cell_size # x & y of the cell left-top corner.\n",
    "            xy_normalized = (center_xy - x0y0) / cell_size # x & y of the box on the cell, normalized from 0.0 to 1.0.\n",
    "            for k in range(B):\n",
    "                s = 5 * k\n",
    "                target[j, i, s  :s+2] = xy_normalized\n",
    "                target[j, i, s+2:s+4] = wh\n",
    "                target[j, i, s+4    ] = 1.0\n",
    "            target[j, i, 5*B + label] = 1.0\n",
    "        return target\n",
    "    \n",
    "    def _get_image(self, idx: int) -> np.ndarray:\n",
    "        image = self.shapes2d.generate_image_by_desc(self.images_info[idx]['bg_color'],\n",
    "                                             self.images_info[idx]['shape_specs'])\n",
    "        return image\n",
    "    \n",
    "    def _get_labels(self, idx: int) -> torch.Tensor:\n",
    "        return torch.Tensor([x[0] for x in self.images_info[idx]['gtruths']])\n",
    "    \n",
    "    def _get_boxes(self, idx: int) -> np.ndarray:\n",
    "        boxes = np.array([x[1] for x in self.images_info[idx]['gtruths']])\n",
    "        return boxes\n",
    "    \n",
    "    def _process_boxes(self, boxes: np.ndarray) -> torch.Tensor:\n",
    "        w, h  = self.img_size, self.img_size\n",
    "        boxes = torch.Tensor(boxes)\n",
    "        boxes /= torch.Tensor([[w, h, w, h]]).expand_as(boxes)\n",
    "        return boxes\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images_info.keys())\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, int]:\n",
    "        \n",
    "        image = self._get_image(idx)\n",
    "        \n",
    "        labels = self._get_labels(idx)\n",
    "        boxes = self._get_boxes(idx)\n",
    "        boxes = self._process_boxes(boxes)\n",
    "        target = self._encode(boxes, labels) # [S, S, 5 x B + C]\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d9a2dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default Generating dataset...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = ShapesDataset(50, 140, 123)\n",
    "train_dataset.show_bboxes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1488d42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cell: [4][2]\n",
      "Boxes:\n",
      "tensor([[0.3500, 0.2000, 0.2857, 0.2857, 1.0000],\n",
      "        [0.3500, 0.2000, 0.2857, 0.2857, 1.0000]])\n",
      "Class confidence: tensor([0., 0., 1.])\n",
      "Name:square\n",
      "\n",
      "Cell: [4][6]\n",
      "Boxes:\n",
      "tensor([[0.2000, 0.9500, 0.3429, 0.3429, 1.0000],\n",
      "        [0.2000, 0.9500, 0.3429, 0.3429, 1.0000]])\n",
      "Class confidence: tensor([0., 1., 0.])\n",
      "Name:triangle\n",
      "\n",
      "Cell: [0][0]\n",
      "Boxes:\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "Class confidence: tensor([0., 0., 0.])\n",
      "Name:empty cell\n"
     ]
    }
   ],
   "source": [
    "# Cells with objects\n",
    "train_dataset.show_cell_info(4, 2, 1)\n",
    "train_dataset.show_cell_info(4, 6, 1)\n",
    "# Empty cell\n",
    "train_dataset.show_cell_info(0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "611dfd12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class YOLO(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone: nn.Module, num_bboxes: int, num_classes: int):\n",
    "        super(YOLO, self).__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.num_cells = 7\n",
    "        self.num_bboxes = num_bboxes\n",
    "        self.num_classes = num_classes\n",
    "        self.output_shape = (-1, self.num_cells, self.num_cells,\n",
    "                             5 * self.num_bboxes + self.num_classes)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=96,\n",
    "                               out_channels=48, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=48,\n",
    "                               out_channels=5 * self.num_bboxes + self.num_classes, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = self.backbone(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = x.view(self.output_shape)\n",
    "        return x\n",
    "\n",
    "    def _inference(self, img: np.ndarray, device: str, transform: Callable) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param img:       image input without preprocessing\n",
    "        :param device:    cpu/gpu\n",
    "        :param transform: callable input preprocessing\n",
    "        :return: [S, S, 5 x B + C] tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if transform:\n",
    "            img = transform(img)\n",
    "        if x.dim() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            out = self.forward(img.to(device))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _get_bbox_scores(self, out: torch.Tensor, box_index: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate cell scores for each bbox by multiplying P(class_i|obj) and P(obj)\n",
    "        By default, box_index={4, 9}: \n",
    "            [x1, y1, w1, h1, p1(objectness)][x2, y2, w2, h2, p2(objectness)]\n",
    "        :param out: inference output\n",
    "        :param box_index: position of objectness score for a bounding box in a cell\n",
    "        \"\"\"\n",
    "        return torch.cat([(out[:, :, 5*2:][:, :, label] * out[:, :, box_index]).unsqueeze(0) \n",
    "                          for label in range(self.num_classes)])\n",
    "    \n",
    "    def _decode(self, out: torch.Tensor, bboxes_scores: tuple or list, \n",
    "                img_size: int, conf_thresh: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        \"\"\"\n",
    "        :param bboxes_scores:  scores, [[class_id, n_cells, n_cells],...]\n",
    "        :param img_size:       input img size\n",
    "        :param conf_thresh: confidence threshold\n",
    "        :return: bounding boxes, scores and class labels\n",
    "        \"\"\"\n",
    "        \n",
    "        out_classes = torch.Tensor([])\n",
    "        out_bboxes = torch.Tensor([])\n",
    "        out_scores = torch.Tensor([])\n",
    "        cell_size = 1 / self.num_cells\n",
    "        \n",
    "        for box_id, box_scores in enumerate(bboxes_scores):\n",
    "            \n",
    "            class_ids, rows, cols = torch.where(box_scores > conf_thresh)\n",
    "            \n",
    "            bboxes = out[rows, cols, 5*box_id: 5*box_id + 4].clone()\n",
    "            bboxes_scores = box_scores[class_ids, rows, cols].clone()\n",
    "\n",
    "            cols_rows = torch.vstack([cols, rows]).T\n",
    "            \n",
    "            x0y0_normalized = cols_rows * cell_size\n",
    "            xy_normalized = bboxes[:, :2] * cell_size + x0y0_normalized\n",
    "            wh_normalized = bboxes[:, 2:]\n",
    "\n",
    "            boxes_xywh = torch.zeros((bboxes.shape[0], 4))\n",
    "            boxes_xywh[:, :2] = xy_normalized - 0.5 * wh_normalized\n",
    "            boxes_xywh[:, 2:] = wh_normalized\n",
    "\n",
    "            boxes_xywh *= torch.repeat_interleave(torch.Tensor([img_size]), 4).expand_as(boxes_xywh)\n",
    "            boxes_xywh = boxes_xywh.ceil()\n",
    "            \n",
    "            out_classes = torch.cat([out_classes, class_ids])\n",
    "            out_bboxes = torch.cat([out_bboxes, boxes_xywh])\n",
    "            out_scores = torch.cat([out_scores, bboxes_scores])\n",
    "        return out_bboxes, out_scores, out_classes\n",
    "            \n",
    "        \n",
    "    def detect(self, img: np.ndarray, conf_thresh: float, \n",
    "               transform: Callable = None, box_ids: tuple = (4, 9)) -> tuple:\n",
    "        \"\"\"\n",
    "        :param img:\n",
    "        :param conf_thresh:\n",
    "        :param transform:\n",
    "        :param box_ids:\n",
    "        \"\"\"\n",
    "        img_size, _ , _ = img.shape\n",
    "        device = next(self.parameters()).device.type\n",
    "        \n",
    "        output = self._inference(img, device, transform)[0].cpu()\n",
    "        boxes_scores = [self._get_bbox_scores(output, b_id) for b_id in box_ids ]\n",
    "        bboxes, scores, classes = self._decode(output, boxes_scores, img_size, conf_thresh) \n",
    "        \n",
    "        bboxes, scores , classes =  \\\n",
    "        bboxes.ceil().numpy().astype(int), scores.numpy(), classes.numpy().astype(int)\n",
    "\n",
    "        return bboxes, scores, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf619e56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mbnet_backbone = torchvision.models.mobilenet_v2(pretrained=True, progress=False)\n",
    "mbnet_backbone = nn.Sequential(*list(mbnet_backbone.features.children())[:-5])\n",
    "yolo = YOLO(backbone=mbnet_backbone, num_bboxes=2, num_classes=3)\n",
    "\n",
    "x = torch.Tensor(np.random.normal(size=(3, 140, 140)))\n",
    "model_path = os.path.join('onnx_graphs', 'custom_yolov1.onnx')\n",
    "torch.onnx.export(yolo, torch.unsqueeze(x, 0), model_path,\n",
    "                  input_names=['input'], output_names=['output'], opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2eae16",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "netron.start(model_path, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c8c3c10",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize((140, 140)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a25d1d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Generating dataset...\n",
      "val Generating dataset...\n",
      "test Generating dataset...\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "for name, n_images, seed in zip(('train', 'val', 'test'),\n",
    "                                (5000, 250, 100),\n",
    "                                (123, 321, 42)):\n",
    "    datasets.update({name:\n",
    "                     ShapesDataset(\n",
    "                         n_images=n_images,\n",
    "                         img_size=140,\n",
    "                         seed=seed,\n",
    "                         transform=torchvision.transforms.Compose(transforms),\n",
    "                         name=name)\n",
    "                     }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "429edad8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = len(datasets['train'].shapes2d.classes)\n",
    "num_boxes = 2\n",
    "feature_size = 7\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "l_rate = 0.001\n",
    "device = 'cuda'\n",
    "\n",
    "train_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66d86346",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Init dataloaders\n",
    "train_loader = DataLoader(datasets['train'], batch_size=batch_size)\n",
    "val_loader = DataLoader(datasets['val'], batch_size=batch_size)\n",
    "test_loader = DataLoader(datasets['test'], batch_size=batch_size)\n",
    "\n",
    "mbnet_backbone = torchvision.models.mobilenet_v2(pretrained=True, progress=False)\n",
    "mbnet_backbone = nn.Sequential(*list(mbnet_backbone.features.children())[:-5])\n",
    "yolo = YOLO(backbone=mbnet_backbone, num_bboxes=2, num_classes=3)\n",
    "yolo = yolo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e868623",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights: ./saved_models/yolo_mbnet/yolo_mbnet_shapes2d_29_1.2412.pt\n"
     ]
    }
   ],
   "source": [
    "criterion = training.YOLOLoss(feature_size=feature_size, num_bboxes=num_boxes, num_classes=n_classes)\n",
    "\n",
    "if train_flag:\n",
    "    optimizer = torch.optim.Adam(yolo.parameters(), lr=l_rate)\n",
    "    training.train_model(train_loader=train_loader,\n",
    "                         val_loader=val_loader,\n",
    "                         dataset_name='shapes2d',\n",
    "                         model=yolo,\n",
    "                         model_name='yolo_mbnet',\n",
    "                         optimizer=optimizer,\n",
    "                         criterion=criterion,\n",
    "                         checkpoint_dir=os.path.join(\n",
    "                             '.', 'saved_models', 'yolo_mbnet'),\n",
    "                         checkpoint_metric='loss',\n",
    "                         epochs=epochs,\n",
    "                         device=device)\n",
    "else:\n",
    "    path = glob.glob(os.path.join('.', 'saved_models',\n",
    "                     'yolo_mbnet', 'yolo_mbnet_shapes2d*.pt'))[0]\n",
    "    print(f'Loading weights: {path}')\n",
    "    yolo.load_state_dict(torch.load(path, map_location=device))\n",
    "    yolo = yolo.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2d23a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Calculate loss for test subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d090a91a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.6911354827880859}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_engine = create_supervised_evaluator(\n",
    "    yolo.to(device),\n",
    "    device=device,\n",
    "    metrics={'loss': Loss(criterion)}\n",
    ")\n",
    "state = test_engine.run(test_loader)\n",
    "state.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b23ec6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shapes2d = utils.RandomShapes(img_size=140, w=40, h=40, seed=512)\n",
    "test_image = shapes2d.get_image()[0]\n",
    "boxes, scores, classes = yolo.detect(test_image, 0.3, torchvision.transforms.Compose(transforms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69c6ac48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for class_idx in np.unique(classes):\n",
    "    img_copy = test_image.copy()\n",
    "    for pts in boxes[classes==class_idx]:\n",
    "        cv2.rectangle(img_copy, pts, (10, 255, 150), 1)\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.imshow(img_copy)\n",
    "    plt.title(f'{shapes2d.classes[class_idx]} detections')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f13946",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### YOLO9000: Better, Faster, Stronger (Redmon J., Farhadi A., 2017)\n",
    "[Paper](https://arxiv.org/abs/1612.08242)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c588afb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "* YOLO main drawbacks:\n",
    "\n",
    "    * A significant number of localization error compared to Fast/Faster RCNN\n",
    "    \n",
    "    * Struggles to detect close objects\n",
    "    \n",
    "    * Low recall \n",
    "    \n",
    "    * One target cell connected to the ground truth bboxes \n",
    "    \n",
    "    \n",
    "* What's new:\n",
    "\n",
    "    * BatchNorm (because of GoogleNet default backbone)\n",
    "    * High resolution classifier, from 224x224 input to 416x416 (to get the odd number of cells and, thus, a single center cell on a grid) with 13x13 feature map (32x downsampling).\n",
    "    * No fully connected layers\n",
    "    * Anchor boxes from Faster RCNN \n",
    "    * Generate prior anchors with k-means GT bboxes (default K=5)\n",
    "    \n",
    "    \n",
    "<img src=\"../assets/6_yolo.png\" width=\"450\">\n",
    "\n",
    "* \n",
    "    * Instead of predicting offsets YOLO still predict location coordinates relative to the location of the grid cell, which still boundshround truth to [0,1].\n",
    "    * Second feature map 26x26 to detect smaller objects\n",
    "    * Multi-scale training with input sizes: {320, 352, ..., 608}\n",
    "    \n",
    "    \n",
    "<img src=\"../assets/4_yolo.png\" width=\"750\">\n",
    "\n",
    "\n",
    "*Note: mAP or mAp@IoU, class averaged area under Precision-Recall curve*\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../assets/5_yolo.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd286c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model inspired researchers to new improvement ideas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6453ecd9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fast YOLO: A fast you only look once system for real-time embedded object detection in video (Shafiee M. J. et al., 2017)\n",
    "[Paper](https://arxiv.org/abs/1709.05943)\n",
    "\n",
    "\n",
    "### YOLOv3: An Incremental Improvement (Redmon J., Farhadi A., 2018)\n",
    "[Paper](https://arxiv.org/abs/1804.02767)\n",
    "\n",
    "\n",
    "\n",
    "### YOLOv4: Optimal Speed and Accuracy of Object Detection (Bochkovskiy A. et al., 2020)\n",
    "[Paper](https://arxiv.org/abs/2004.10934)\n",
    "\n",
    "\n",
    "\n",
    "### YOLOv5 (Glenn Jocher, 2020)\n",
    "[GitHub](https://github.com/ultralytics/yolov5)\n",
    "\n",
    "\n",
    "\n",
    "### YOLOX: Exceeding YOLO Series in 2021 (Ge Z. et al., 2021)\n",
    "[Paper](https://arxiv.org/abs/2107.08430)\n",
    "\n",
    "\n",
    "Architectures based on experiments with [PaddlePaddle](https://github.com/PaddlePaddle/Paddle) ML platfrom\n",
    "\n",
    "### PP-YOLO: An Effective and Efficient Implementation of Object Detector (Long X. et al., 2020)\n",
    "[Paper](https://arxiv.org/abs/2007.12099)\n",
    "\n",
    "\n",
    "### PP-YOLOv2: A Practical Object Detector (Huang X. et al., 2021)\n",
    "[Paper](https://arxiv.org/abs/2104.10419)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be2a43",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References\n",
    "\n",
    "* https://docs.ultralytics.com/#the-history-of-yolo\n",
    "* https://github.com/ultralytics/yolov5\n",
    "* https://github.com/PaddlePaddle/Paddle/tree/develop/doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
