{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9cdb0a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  ResNet  \n",
    "\n",
    "### Deep Residual Learning for Image Recognition (He K. et al., 2016)\n",
    "\n",
    "*We explicitly reformulate the layers as learning __residual functions__ with reference to the layer inputs, instead of learning unreferenced functions...\n",
    "We provide comprehensive empirical evidence showing that these __residual\n",
    "networks are easier to optimize, and can gain accuracy from\n",
    "considerably increased depth__.*\n",
    "\n",
    "\n",
    "[Paper](https://arxiv.org/abs/1512.03385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41accb00",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import netron\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "assert torch.cuda.is_available() is True\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a371008",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch : 1.10.2\n",
      "ignite: 0.4.8\n",
      "numpy : 1.22.1\n",
      "netron: 5.7.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -p torch,ignite,numpy,netron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e208d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "*When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated...*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e887c26",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/2_resnet.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb7315",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/5_resnet.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ce121",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Proposed basic block (left and right):\n",
    "\n",
    "$$y = \\mathcal{F}(\\mathbf{x}, \\{W_i\\}) + \\mathbf{x} $$\n",
    "\n",
    "Projection block for dimensions matching (+ 1x1 convolution in skip connection):\n",
    "\n",
    "\n",
    "$$y = \\mathcal{F}(\\mathbf{x}, \\{W_i\\}) + W_s\\mathbf{x} $$\n",
    "\n",
    "The usage of bottleneck designs is mainly due to practical considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62cac45c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel: int, pad: int = 0, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size=kernel, padding=pad, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.bn(self.conv(x))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca868c18",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BuildingBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = BasicConv2d(in_channels, in_channels, 3, 1)\n",
    "        self.conv2 = BasicConv2d(in_channels, out_channels, 3, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.relu(self.conv1(x))\n",
    "        z = self.conv2(z)\n",
    "        z += x\n",
    "        print(f'Total feature maps: {z.shape[1]} of size: {z.shape[2:]}')\n",
    "        return self.relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a3cc772",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BuildingBlock(\n",
       "  (conv1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv2): BasicConv2d(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18_conv2x = BuildingBlock(64, 64)\n",
    "resnet18_conv2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66f5d73",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature maps: 64 of size: torch.Size([60, 60])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(np.random.normal(size=(1, 64, 60, 60)))\n",
    "model_path = os.path.join('onnx_graphs', 'resnet18_conv2x.onnx')\n",
    "torch.onnx.export(resnet18_conv2x, x, model_path,\n",
    "                  input_names=['input'], output_names=['output'], opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049c8c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "netron.start(model_path, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bde61ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_conv_ops(kernel, output_channel, input_shape):\n",
    "    return np.prod([*kernel, output_channel, *input_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0bd2fbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132710400 132710400]\n",
      "[0.5 0.5]\n",
      "Total operations: 265.421 M\n"
     ]
    }
   ],
   "source": [
    "ops_cnt = np.array((count_conv_ops((3, 3), 64, (64, 60, 60)),\n",
    "                    count_conv_ops((3, 3), 64, (64, 60, 60)),))\n",
    "print(\"%s\\n%s\" % (ops_cnt, ops_cnt/np.sum(ops_cnt)))\n",
    "print('Total operations: %.3f M' % (np.sum(ops_cnt)/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5631ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: list or tuple, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.branch1 = nn.Sequential(BasicConv2d(in_channels, out_channels[0], 1),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     BasicConv2d(out_channels[0], out_channels[1], 3, 1),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     BasicConv2d(out_channels[1], out_channels[2], 1))\n",
    "        \n",
    "        self.branch2 = nn.Sequential(BasicConv2d(in_channels, out_channels[2], 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.branch1(x) \n",
    "        sc = self.branch2(x) \n",
    "        z += sc\n",
    "        print(f'Total feature maps: {z.shape[1]} of size: {z.shape[2:]}')\n",
    "        return self.relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d583c863",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BottleneckBlock(\n",
       "  (branch1): Sequential(\n",
       "    (0): BasicConv2d(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): BasicConv2d(\n",
       "      (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (branch2): Sequential(\n",
       "    (0): BasicConv2d(\n",
       "      (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_conv2x = BottleneckBlock(64, (64, 64, 256))\n",
    "resnet50_conv2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20040369",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature maps: 256 of size: torch.Size([60, 60])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(np.random.normal(size=(1, 64, 60, 60)))\n",
    "model_path = os.path.join('onnx_graphs', 'resnet50_conv2x.onnx')\n",
    "torch.onnx.export(resnet50_conv2x, x, model_path, \n",
    "                  input_names=['input'], output_names=['output'], opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa839b9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'onnx_graphs/resnet50_conv2x.onnx' at http://localhost:30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 30000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netron.start(model_path, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4490e22c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14745600 132710400  58982400]\n",
      "[0.07142857 0.64285714 0.28571429]\n",
      "Total operations: 206.438 M\n"
     ]
    }
   ],
   "source": [
    "ops_cnt = np.array((count_conv_ops((1, 1), 64, (64, 60, 60)),\n",
    "                    count_conv_ops((3, 3), 64, (64, 60, 60)),\n",
    "                    count_conv_ops((1, 1), 256, (64, 60, 60)),\n",
    "                   ))\n",
    "print(\"%s\\n%s\" % (ops_cnt, ops_cnt/np.sum(ops_cnt)))\n",
    "print('Total operations: %.3f M' % (np.sum(ops_cnt)/1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd050df9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So, bottleneck layers first decrease the number of feature maps and restore it in the output.\n",
    "\n",
    "How can we drop out many features so carelessly?\n",
    "\n",
    "The structure of the image data: lots of correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ced494",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/4_resnet.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591336c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"../assets/6_resnet.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557ea3b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ResNet interpretations:\n",
    "\n",
    "* A system of simultaneously parallel and serial modules: in many models, the in-out signal comes in parallel, and the output signals of each module are connected in series. An ansamble of parallel and series modules: [Link](https://arxiv.org/abs/1605.06431)\n",
    "\n",
    "* It is related to the visual cortex models: [Link](https://arxiv.org/abs/1604.03640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8ac5c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Visualizing the Loss Landscape of Neural Nets (Li, Hao, et al., 2018)\n",
    "[Paper](https://arxiv.org/pdf/1712.09913.pdf)\n",
    "\n",
    "\n",
    "<img src=\"../assets/1_resnet.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a33d4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Torch [implementation](https://github.com/pytorch/vision/blob/e13206d9749e81fd8b3aec5e664f697a73febf9f/torchvision/models/resnet.py#L164)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610b485",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```python\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de948d1f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Modified Bottleneck layer (resnet{50,101,152}):\n",
    "\n",
    "\n",
    "```python\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "* Zero init of the last BN improves accuracy:\n",
    "\n",
    "```python\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        \n",
    "...\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6811a2c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('resnet', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(arch for arch in dir(torchvision.models) if re.match('resnet', arch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5652622",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Your training code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779b728",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define data transformation pipeline.\n",
    "\n",
    "\n",
    "# Initialize dataset and dataloaders.\n",
    "\n",
    "\n",
    "# Initialize pretrained network, replace Linear layer with a new one for your dataset.\n",
    "\n",
    "\n",
    "# Initialize optimizer, loss function and training procedure with handlers/callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5553f0fc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References\n",
    "\n",
    "* http://cs231n.stanford.edu/slides/2021/lecture_9.pdf\n",
    "* https://onnx.ai/\n",
    "* https://pytorch.org/docs/stable/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
