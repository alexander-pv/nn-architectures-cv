{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9daa3a12",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SSD object detection architecture\n",
    "\n",
    "### SSD: Single Shot MultiBox Detector (Liu W. et al., 2016)\n",
    "[Paper](https://arxiv.org/abs/1512.02325)\n",
    "\n",
    "*Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97989c2a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "assert torch.cuda.is_available() is True\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07ae993",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch      : 1.10.2\n",
      "torchvision: 0.11.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -p torch,torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41014cd9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Comparing to YOLOv1:\n",
    "\n",
    "* Default boxes mechanism (Anchor boxes from RCNN):\n",
    "\n",
    "<img src=\"../assets/4_ssd.png\" width=\"550\">\n",
    "\n",
    "\n",
    "* Moving back to background class (RCNN)\n",
    "\n",
    "\n",
    "* Offsets $(t_x, t_y, t_w, t_h)$ prediction (RCNN)\n",
    "\n",
    "\n",
    "* Thus, we need to encode 4(offsets) + C (number of classes) informatiuon for each default box\n",
    "\n",
    "\n",
    "* Multiple feature maps to detect objects with vairous scales in an image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d56fe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Paper figure. SSD vs YOLO:\n",
    "\n",
    "<img src=\"../assets/3_ssd.png\" width=\"850\">\n",
    "\n",
    "\n",
    "SSD300 with VGG backbone:\n",
    "\n",
    "<img src=\"../assets/2_ssd.svg\" width=\"950\">\n",
    "\n",
    "\n",
    "*Note: different versions of the paper describe different default boxes for VGG16:* \n",
    "\n",
    "* 7308: (3, 6, 6, 6, 6, 6) prior boxes\n",
    "* 8732: (4, 6, 6, 6, 4, 4) prior boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f3625",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Preparing default boxes / anchors:\n",
    "\n",
    "\n",
    "Anchor positions for a feature map $m x n$:\n",
    "\n",
    "$$(\\frac{i+0.5}{m}, \\frac{j+0.5}{n}), i=0,1...,m; j=0,1,...,n$$\n",
    "\n",
    "Matching anchors to GT:\n",
    "\n",
    "$anchor=\\left\\{ \n",
    "  \\begin{array}{ c l }\n",
    "    positive, & \\quad IoU_{(anchor, GT)} \\ge 0.5 \\\\\n",
    "    negative (background), & \\quad IoU_{(anchor, GT)} < 0.5\n",
    "  \\end{array}\n",
    "\\right.$\n",
    "\n",
    "\n",
    "\n",
    "So, unlike YOLOv1, multiple default boxes may be marked as GT for an object in an image.\n",
    "\n",
    "\n",
    "\n",
    "* 3 anchors for the largest feature map, 6 anchors  - for the rest\n",
    "\n",
    "* Model loss:\n",
    "\n",
    "\n",
    "$I_{i,j} = [IoU(GT_j, anchor_i) \\ge 0.5]$,\n",
    "\n",
    "$N = \\sum_{i, j} I_{i,j}$, the number of matched default boxes,\n",
    "\n",
    "\n",
    "$$Loss = \\frac{1}{N}[L_{class} + \\alpha L_{loc}],$$\n",
    "\n",
    "\n",
    "* $ L_{loc}$ is $ L_{loc}$ from RCNN,\n",
    "\n",
    "\n",
    "* $ L_{class} $ is a bit tricky:\n",
    "\n",
    "    * So many default boxes produces a lot of negative examples. This is a serious problem. Because of that the model could be trained predicting only backgrounds.\n",
    "    \n",
    "    * To compute $ L_{class}$ we collect positive examples and only negative ones with highest non-background scores (detector predicted them as any object, \"hard negative mining\") keeping the ratio $\\frac{neg}{pos}=\\frac{3}{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b4a98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training scheme:\n",
    "\n",
    "<img src=\"../assets/5_ssd.svg\" width=\"450\">\n",
    "\n",
    "\n",
    "Inference scheme:\n",
    "\n",
    "<img src=\"../assets/6_ssd.svg\" width=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e12ad9e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ssd', 'ssd300_vgg16', 'ssdlite', 'ssdlite320_mobilenet_v3_large']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(torchvision.models.detection) if 'ssd' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f4abdc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Implements SSD architecture from `\"SSD: Single Shot MultiBox Detector\" <https://arxiv.org/abs/1512.02325>`_.\n",
      "\n",
      "    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n",
      "    image, and should be in 0-1 range. Different images can have different sizes but they will be resized\n",
      "    to a fixed size before passing it to the backbone.\n",
      "\n",
      "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
      "\n",
      "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
      "    containing:\n",
      "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n",
      "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
      "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
      "\n",
      "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
      "    losses.\n",
      "\n",
      "    During inference, the model requires only the input tensors, and returns the post-processed\n",
      "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
      "    follows, where ``N`` is the number of detections:\n",
      "\n",
      "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n",
      "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
      "        - labels (Int64Tensor[N]): the predicted labels for each detection\n",
      "        - scores (Tensor[N]): the scores for each detection\n",
      "\n",
      "    Args:\n",
      "        backbone (nn.Module): the network used to compute the features for the model.\n",
      "            It should contain an out_channels attribute with the list of the output channels of\n",
      "            each feature map. The backbone should return a single Tensor or an OrderedDict[Tensor].\n",
      "        anchor_generator (DefaultBoxGenerator): module that generates the default boxes for a\n",
      "            set of feature maps.\n",
      "        size (Tuple[int, int]): the width and height to which images will be rescaled before feeding them\n",
      "            to the backbone.\n",
      "        num_classes (int): number of output classes of the model (including the background).\n",
      "        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
      "            They are generally the mean values of the dataset on which the backbone has been trained\n",
      "            on\n",
      "        image_std (Tuple[float, float, float]): std values used for input normalization.\n",
      "            They are generally the std values of the dataset on which the backbone has been trained on\n",
      "        head (nn.Module, optional): Module run on top of the backbone features. Defaults to a module containing\n",
      "            a classification and regression module.\n",
      "        score_thresh (float): Score threshold used for postprocessing the detections.\n",
      "        nms_thresh (float): NMS threshold used for postprocessing the detections.\n",
      "        detections_per_img (int): Number of best detections to keep after NMS.\n",
      "        iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
      "            considered as positive during training.\n",
      "        topk_candidates (int): Number of best detections to keep before NMS.\n",
      "        positive_fraction (float): a number between 0 and 1 which indicates the proportion of positive\n",
      "            proposals used during the training of the classification head. It is used to estimate the negative to\n",
      "            positive ratio.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(torchvision.models.detection.SSD.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63e83d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### DSSD: Deconvolutional single shot detector (Fu C. Y. et al., 2017)\n",
    "[Paper](https://arxiv.org/abs/1701.06659)\n",
    "\n",
    "### FSSD: Feature Fusion Single Shot Multibox Detector (Li Z., Zhou, 2017)\n",
    "[Paper](https://arxiv.org/abs/1712.00960)\n",
    "\n",
    "### ASSD: Attentive single shot multibox detector (Yi J., Wu P., Metaxas D. N., 2019)\n",
    "[Paper](https://arxiv.org/abs/1909.12456)\n",
    "\n",
    "<img src=\"../assets/1_ssd.png\" width=\"550\">\n",
    "\n",
    "### U-SSD: Improved SSD Based on U-Net Architecture for End-to-End Table Detection in Document Images (Lee S. H., Chen H. C., 2021)\n",
    "[Paper](https://www.mdpi.com/2076-3417/11/23/11446/htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8b2ae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References\n",
    "\n",
    "* https://paperswithcode.com/method/ssd\n",
    "* https://pytorch.org/blog/torchvision-ssd-implementation/\n",
    "* https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/\n",
    "* https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
